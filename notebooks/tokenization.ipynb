{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install datasets sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install amseg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pdf2image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pytesseract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/labeled/labeled_telegram_product_price_location.txt', 'r', encoding='utf-8') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "    # Process lines as needed\n",
    "data = [line.strip().split('\\t') for line in lines]  # Adjust the split based on your delimiter\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process lines: separate tokens and labels\n",
    "data = [line.strip().split() for line in lines if line.strip()]  # Split based on spaces\n",
    "tokens = [item[0] for item in data]  # Extract tokens\n",
    "labels = [item[1] for item in data]  # Extract labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the tokenizer\n",
    "model_name = \"xlm-roberta-base\"  # Change to any approprate model from Hugging Face if needed\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to tokenize and align labels\n",
    "def tokenize_and_align_labels(tokenizer, tokens, labels):\n",
    "    aligned_tokens = []\n",
    "    aligned_labels = []\n",
    "\n",
    "    # Tokenize each word/token with associated label\n",
    "    for word, label in zip(tokens, labels):\n",
    "        tokenized_word = tokenizer.tokenize(word)  # Tokenize the word\n",
    "        aligned_tokens.extend(tokenized_word)  # Add tokens to the list\n",
    "\n",
    "        # Assign the label to the first subtoken and 'O' to subsequent subtokens\n",
    "        aligned_labels.extend([label] + ['O'] * (len(tokenized_word) - 1))\n",
    "\n",
    "    return aligned_tokens, aligned_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and align labels\n",
    "aligned_tokens, aligned_labels = tokenize_and_align_labels(tokenizer, tokens, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first 10 results\n",
    "for token, label in zip(aligned_tokens[:20], aligned_labels[:20]):\n",
    "    print(f\"{token:20} {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Amharic segmenter\n",
    "sent_punct = []\n",
    "word_punct = []\n",
    "segmenter = AmharicSegmenter(sent_punct, word_punct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process lines: separate tokens and labels\n",
    "data = [line.strip().split() for line in lines if line.strip()]  # Split based on spaces\n",
    "tokens = [item[0] for item in data]  # Extract tokens\n",
    "labels = [item[1] for item in data]  # Extract labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to align tokens with their respective labels\n",
    "def align_tokens_with_labels(tokenizer, tokens, labels):\n",
    "    aligned_tokens = []\n",
    "    aligned_labels = []\n",
    "\n",
    "    # Tokenize each word/token with associated label\n",
    "    for word, label in zip(tokens, labels):\n",
    "        tokenized_word = tokenizer.amharic_tokenizer(word)  # Tokenize the word\n",
    "        aligned_tokens.extend(tokenized_word)  # Add tokens to the list\n",
    "\n",
    "        # Assign the label to the first subtoken and 'O' to subsequent subtokens\n",
    "        aligned_labels.extend([label] + ['O'] * (len(tokenized_word) - 1))\n",
    "\n",
    "    return aligned_tokens, aligned_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage with different variable names\n",
    "new_tokens, new_labels = align_tokens_with_labels(segmenter, tokens, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output the first 20 aligned tokens and labels\n",
    "for token, label in zip(new_tokens[:20], new_labels[:20]):\n",
    "    print(f\"{token}: {label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the final tokens and labels to a CSV file\n",
    "output_df = pd.DataFrame({'Token': new_tokens, 'Label': new_labels})\n",
    "output_df.to_csv('/data/processed/final_tokens_labels.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
